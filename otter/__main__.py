from itertools import count
from collections import defaultdict, Counter
import otf2
import igraph as ig
import otter
from otter.utils import VertexLabeller, VertexAttributeCombiner, AttributeHandlerTable
from otter.definitions import RegionType, Endpoint, EdgeType

args = otter.get_args()
otter.log.initialise(args)
log = otter.log.get_logger("main")

log.info(f"reading OTF2 anchorfile: {args.anchorfile}")
with otf2.reader.open(args.anchorfile) as r:
    events = otter.EventFactory(r)
    tasks = otter.TaskRegistry()
    log.info(f"generating chunks")
    chunks = otter.ChunkFactory(events, tasks).chunks
    graphs = list(chunk.graph for chunk in chunks)

# Dump chunks and graphs to log file
if args.loglevel == "DEBUG":
    chunk_log = otter.log.get_logger("chunks_debug")
    graph_log = otter.log.get_logger("graphs_debug")
    task_log = otter.log.get_logger("tasks_debug")

    graph_log.debug(">>> BEGIN GRAPHS <<<")
    chunk_log.debug(f">>> BEGIN CHUNKS <<<")

    for chunk in chunks:

        # write chunk
        for line in chunk.to_text():
            chunk_log.debug(f"{line}")

        # write graph
        graph_log.debug(f"Chunk type: {chunk.type}")
        g = chunk.graph
        lines = [" ".join(f"{g}".split("\n"))]
        for line in lines:
            graph_log.debug(f"{line}")
        for v in g.vs:
            graph_log.debug(f"{v}")
        graph_log.debug("")

    chunk_log.debug(f">>> END CHUNKS <<<")
    graph_log.debug(">>> END GRAPHS <<<")

    task_log.debug(">>> BEGIN TASKS <<<")
    attributes = ",".join(tasks.attributes)
    task_log.debug(f"{attributes=}")
    for record in tasks.data:
        task_log.debug(f"{record}")
    task_log.debug(">>> END TASKS <<<")

# Collect all chunks
log.info("combining chunks")
g = ig.disjoint_union([c.graph for c in chunks])
vcount = g.vcount()
log.info(f"graph disjoint union has {vcount} vertices")

# Define some vertex attributes
for name in ['_task_cluster_id', '_is_task_enter_node', '_is_task_leave_node', '_region_type', '_master_enter_event', '_taskgroup_enter_event']:
    if name not in g.vs.attribute_names():
        g.vs[name] = None

# Define some edge attributes
for name in [otter.Attr.edge_type]:
    if name not in g.es.attribute_names():
        g.es[name] = None

# Create vertex labellers
log.info("creating vertex labellers")
parallel_vertex_labeller = VertexLabeller(otter.utils.key_is_not_none('_parallel_sequence_id'), group_key='_parallel_sequence_id')
single_vertex_labeller = VertexLabeller(otter.utils.is_single_executor, group_key='event')
master_vertex_labeller = VertexLabeller(otter.utils.is_master, group_key='event')
task_vertex_labeller = VertexLabeller(otter.utils.key_is_not_none('_task_cluster_id'), group_key='_task_cluster_id')
empty_task_vertex_labeller = VertexLabeller(otter.utils.is_empty_task_region, group_key=lambda v: v['_task_cluster_id'][0])

# Make a table for mapping vertex attributes to handlers - used by ig.Graph.contract_vertices
handlers = AttributeHandlerTable(g.vs.attribute_names(), level=otter.log.DEBUG)

# Supply the logic to use when combining each of these vertex attributes
attribute_handlers = [
    ("_master_enter_event", otter.utils.handlers.return_unique_master_event, (type(None), otter.core.events._Event)),
    ("_task_cluster_id",    otter.utils.handlers.pass_the_unique_value,      (type(None), tuple)),
    ("_is_task_enter_node", otter.utils.handlers.pass_bool_value,            (type(None), bool)),
    ("_is_task_leave_node", otter.utils.handlers.pass_bool_value,            (type(None), bool))
]
for attribute, handler, accept in attribute_handlers:
    handlers[attribute] = VertexAttributeCombiner(handler, accept=accept, msg=f"combining attribute: {attribute}")

log.info(f"combining vertices...")

"""
Contract vertices according to _parallel_sequence_id to combine the chunks generated by the threads of a parallel block.
When combining the 'event' vertex attribute, keep single-executor events over single-other events. All other events
should be combined in a list. 
"""
log.info(f"combining vertices by parallel sequence ID")
handlers['event'] = VertexAttributeCombiner(otter.utils.handlers.return_unique_single_executor_event)
labeller = VertexLabeller(otter.utils.key_is_not_none('_parallel_sequence_id'), group_key='_parallel_sequence_id')
g.contract_vertices(labeller.label(g.vs), combine_attrs=handlers)
vcount_prev, vcount = vcount, g.vcount()
log.info(f"vertex count updated: {vcount_prev} -> {vcount}")

"""
Contract those vertices which refer to the same single-executor event. This connects single-executor chunks to the
chunks containing them, as both chunks contain references to the single-exec-begin/end events.
"""
log.info(f"combining vertices by single-begin/end event")
# group_key needs to return a bare event from a list of 1 event list because the list can't be used as a dict key in VertexLabeller.label()...
labeller = VertexLabeller(otter.utils.is_single_executor, group_key=lambda vertex: vertex['event'][0])
g.contract_vertices(labeller.label(g.vs), combine_attrs=handlers)
vcount_prev, vcount = vcount, g.vcount()
log.info(f"vertex count updated: {vcount_prev} -> {vcount}")

"""
master-leave vertices (which refer to their master-leave event) refer to their corresponding master-enter event.
"""
log.info(f"combining vertices by master-begin/end event")
handlers['event'] = VertexAttributeCombiner(otter.utils.handlers.return_unique_master_event)
labeller = VertexLabeller(otter.utils.is_master, group_key='event')
g.contract_vertices(labeller.label(g.vs), combine_attrs=handlers)
vcount_prev, vcount = vcount, g.vcount()
log.info(f"vertex count updated: {vcount_prev} -> {vcount}")


"""
Intermediate clean-up: for each master region, remove edges that connect
the same nodes as the master region
"""
master_enter_vertices = filter(lambda vertex: isinstance(vertex['event'], otter.core.events.MasterBegin), g.vs)
master_leave_vertices = filter(lambda vertex: isinstance(vertex['event'], otter.core.events.MasterEnd), g.vs)
master_enter_vertex_map = {enter_vertex['event']: enter_vertex for enter_vertex in master_enter_vertices}
master_vertex_pairs = ((master_enter_vertex_map[leave_vertex['_master_enter_event']], leave_vertex) for leave_vertex in master_leave_vertices)
neighbour_pairs = {(enter.predecessors()[0], leave.successors()[0]) for enter, leave in master_vertex_pairs}
redundant_edges = list(filter(lambda edge: (edge.source_vertex, edge.target_vertex) in neighbour_pairs, g.es))
log.info(f"deleting redundant edges due to master regions: {len(redundant_edges)}")
g.delete_edges(redundant_edges)

"""
Contract by _task_cluster_id, rejecting task-create vertices to replace task-create vertices with the corresponding
task's chunk.
"""
log.info("combining vertices by task ID & endpoint")
handlers['event'] = VertexAttributeCombiner(otter.utils.handlers.reject_task_create)
labeller = VertexLabeller(otter.utils.key_is_not_none('_task_cluster_id'), group_key='_task_cluster_id')
g.contract_vertices(labeller.label(g.vs), combine_attrs=handlers)
vcount_prev, vcount = vcount, g.vcount()
log.info(f"vertex count updated: {vcount_prev} -> {vcount}")

"""
Contract vertices with the same task ID where the task chunk contains no internal vertices to get 1 vertex per empty
task region.
"""
log.info("combining vertices by task ID where there are no nested nodes")
handlers['_task_cluster_id'] = VertexAttributeCombiner(otter.utils.handlers.pass_the_set_of_values, accept=tuple, msg="combining attribute: _task_cluster_id")
labeller = VertexLabeller(otter.utils.is_empty_task_region, group_key=lambda v: v['_task_cluster_id'][0])
g.contract_vertices(labeller.label(g.vs), combine_attrs=handlers)
vcount_prev, vcount = vcount, g.vcount()
log.info(f"vertex count updated: {vcount_prev} -> {vcount}")
#         v['task_id'] = v['task_cluster_id'][0]


"""
Contract pairs of directly connected vertices which represent empty barriers, taskwait & loop regions.  
"""
log.info("combining redundant sync and loop enter/leave node pairs")
handlers['event'] = VertexAttributeCombiner(otter.utils.handlers.pass_args)
labeller= VertexLabeller(otter.utils.key_is_not_none('_sync_cluster_id'), group_key='_sync_cluster_id')
g.contract_vertices(labeller.label(g.vs), combine_attrs=handlers)
vcount_prev, vcount = vcount, g.vcount()
log.info(f"vertex count updated: {vcount_prev} -> {vcount}")

# Unpack the region_type attribute


"""
        Apply taskwait synchronisation
        ==============================

1. Give each explicit task a reference to the last vertex which represents its chunk. This is the vertex which will be
connected to the corresponding taskwait vertex, if applicable. The correct vertex will contain exactly 1 TaskSwitch(complete)
event for the corresponding task.
"""
log.debug(f"filtering for task-complete vertices")
task_complete_vertices = dict()
for vertex in filter(otter.utils.is_terminal_task_vertex, g.vs):
    event = vertex['event']
    if isinstance(event, list):
        event = otter.utils.handlers.return_unique_taskswitch_complete_event(event)
    assert otter.core.is_event(event)
    log.debug(f" - task {event.prior_task_id}: {event}")
    task_complete_vertices[event.prior_task_id] = vertex

log.info("applying taskwait synchronisation")

"""
2. For each task that encountered a taskwait barrier, make a list of the taskwait-begin/end event pairs & the vertex
which refers to these events
"""
log.debug(f"gathering taskwait vertices by encountering task ID")
taskwait_vertices = defaultdict(list)
for vertex in filter(otter.utils.is_taskwait, g.vs):
    log.debug(f"got taskwait vertex {vertex} with events:")
    for event in vertex['event']: # now guaranteed to have exactly 2 event instances per vertex (tw-begin+end)
        log.debug(f" - {event}")
        taskwait_vertices[event.encountering_task_id].append(dict(event=event, vertex=vertex))

"""
3. For each task that encountered a taskwait barrier, connect child tasks to the correct taskwait barrier (if any).
"""
for task_id, event_vertex_dicts in taskwait_vertices.items():

    log.debug(f"applying taskwait synchronisation for children of task {task_id}: {list(tasks[task_id].children)}")

    """
    Keep the event-vertex pairs for which the event is the taskwait-enter event
    """
    event_vertex_dicts = sorted(filter(lambda d: d['event'].endpoint == Endpoint.enter, event_vertex_dicts), key=lambda d: d['event'].time)
    log.debug(f"task {task_id} encountered {(len(event_vertex_dicts))} taskwait barriers:")
    for record in event_vertex_dicts:
        log.debug(f" - task {task_id} encountered {record['event']} {record['vertex']}")

    """Iterate over the taskwait-enter events for this taskID, in chronological order"""
    event_vertex_pairs_iter = iter(event_vertex_dicts)

    """Iterate over the children of this task in the order they were created"""
    children_iter = iter(sorted(tasks[task_id].children, key=lambda id: tasks[id].crt_ts))

    """Get the first pair of taskwait-enter events"""
    previous_event, previous_vertex = None, None
    next_record = next(event_vertex_pairs_iter, None)
    next_event = next_record['event'] if next_record else None
    next_vertex = next_record['vertex'] if next_record else None


    while True:

        """Get the next child task if any are left to be synchronised"""
        try:
            child = next(children_iter)
        except StopIteration:
            """ran out of child tasks"""
            break

        """Assert: expect that we haven't already missed the taskwait barrier for this child task"""
        if previous_event and tasks[child].crt_ts < previous_event.time:
            print(tasks[child])
            print(previous_event)
            raise ValueError("child created before previous")

        """This child is synchronised by either "next_event" or a subsequent taskwait barrier"""
        while next_event and next_event.time <= tasks[child].crt_ts:
            previous_event, previous_vertex = next_event, next_vertex
            next_record = next(event_vertex_pairs_iter, None)
            next_event = next_record['event'] if next_record else None
            next_vertex = next_record['vertex'] if next_record else None

        if next_event is None:
            """Ran out of taskwait barriers in the parent task which could synchronise the child task"""
            break

        task_complete_vertex = task_complete_vertices[child]
        assert next_event.time > tasks[child].crt_ts and (previous_event is None or previous_event.time < tasks[child].crt_ts)
        edge = g.add_edge(task_complete_vertex, next_vertex)
        edge['edge_type'] = EdgeType.taskwait
        log.debug(f"synchronised task {child}:")
        log.debug(f"  from: {task_complete_vertex}")
        log.debug(f"    to: {next_vertex}")

        if previous_event is None and next_event is None:
            """ran out of taskwait barriers - no further taskwait synchronisation to apply to children of this task"""
            break

del taskwait_vertices

"""
        Apply taskgroup synchronisation
        ===============================

For each task that encountered a taskgroup region, gather a list of the taskgroup begin/end events and the vertex which
represents the taskgroup-end event.
"""

log.info("applying taskgroup synchronisation")

log.debug(f"gathering taskgroup regions by encountering task ID")
taskgroup_vertices = defaultdict(list)
for vertex in filter(otter.utils.is_task_group_end_vertex, g.vs):
    end_event = vertex['event']
    if isinstance(end_event, list):
        end_event = otter.utils.handlers.return_unique_taskgroup_complete_event(end_event)
    assert otter.core.EventFactory.events.is_event(end_event)
    begin_event = vertex['_taskgroup_enter_event']
    assert begin_event is not None and otter.core.EventFactory.events.is_event(begin_event)
    taskgroup_vertices[end_event.encountering_task_id].append((begin_event, end_event, vertex))
    log.debug(f" - task {end_event.encountering_task_id}: {(begin_event, end_event, vertex)}")


"""
For each task that encountered any taskgroup regions:
"""
was_created_during = lambda crt_ts: (lambda items: items[0].time < crt_ts < items[1].time)
for task_id, taskgroup_vertex_pairs in taskgroup_vertices.items():
    descendants = tasks.descendants_while(task_id, lambda t : t.task_type != TaskType.implicit)
    log.debug(f"applying taskgroup synchronisation for descendants of task {task_id} {descendants}")
    log.debug(f"task {task_id} encountered {(len(taskgroup_vertex_pairs))} taskgroup regions")

    """
    For each descendant task (stopping at descendants which are implicit tasks...
    """
    for desc_id in descendants:
        desc_task = tasks[desc_id]
        match = list(filter(was_created_during(desc_task.crt_ts), taskgroup_vertex_pairs))
        if len(match) == 0:
            log.debug(f" - task {desc_id} {desc_task.crt_ts=} not synchronised by taskgroup region")
        else:
            assert len(match) == 1
            *_, tg_end_vertex = match[0]
            edge = g.add_edge(task_complete_vertices[desc_id], tg_end_vertex)
            edge['edge_type'] = EdgeType.taskgroup
            log.debug(f" - task {desc_id} synchronised by taskgroup-end event at {tg_end_vertex['event'].time}")


g.simplify(combine_edges='first')

if args.output:
    log.info(f"writing graph to {args.output}")
    import warnings
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")
        try:
            g.write(args.output)
        except OSError as oserr:
            print(f"igraph error: {oserr}")
            print(f"failed to write to file '{args.output}'")

# Unpack vertex event attributes
for vertex in g.vs:
    event = vertex['event']
    log.debug(f"unpacking vertex {event=}")
    attributes = otter.core.events.unpack(event)
    for key, value in attributes.items():
        log.debug(f"  got {key}={value}")
        if isinstance(value, list):
            s = set(value)
            if len(s) == 1:
                value = s.pop()
            else:
                log.debug(f"  concatenate {len(value)} values")
                value = ";".join(str(item) for item in value)
        if isinstance(value, int):
            value = str(value)
        elif value == "":
            value = None
        log.debug(f"    unpacked {value=}")
        vertex[key] = value

# Dump graph details to file
if args.loglevel == "DEBUG":
    log.info(f"writing graph to graph.log")
    with open("graph.log", "w") as f:
        f.write("### VERTEX ATTRIBUTES:\n")
        for name in g.vs.attribute_names():
            levels = set(otter.utils.flatten(g.vs[name]))
            n_levels = len(levels)
            if n_levels <= 6:
                f.write(f"  {name:>35} {n_levels:>6} levels {list(levels)}\n")
            else:
                f.write(f"  {name:>35} {n_levels:>6} levels (...)\n")

        # Counter = otter.utils.counters.PrettyCounter(value for value in g.vs['region_type'])

        f.write("\nCount of vertex['event'] types:\n")
        f.write(str(Counter))
        f.write("\n\n")

        f.write("### EDGE ATTRIBUTES:\n")
        for name in g.es.attribute_names():
            levels = set(otter.utils.flatten(g.es[name]))
            n_levels = len(levels)
            if n_levels <= 6:
                f.write(f"  {name:>35} {n_levels:>6} levels ({list(levels)})\n")
            else:
                f.write(f"  {name:>35} {n_levels:>6} levels (...)\n")

        f.write("\n")

        f.write("### VERTICES:\n")
        for v in g.vs:
            f.write(f"{v}\n")

        f.write("\n")
        f.write("### EDGES:\n")
        for e in g.es:
            f.write(f"{e.tuple}\n")

# Clean up temporary vertex attributes
for name in g.vs.attribute_names():
    if name.startswith("_"):
        del g.vs[name]

del g.vs['event']

if args.report:
    otter.styling.style_graph(g)
    otter.styling.style_tasks(tasks.task_tree())
    otter.reporting.write_report(args, g, tasks)

if args.interact:
    otter.interact(locals(), g)

log.info("Done!")
