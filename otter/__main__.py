from itertools import count
from collections import defaultdict, Counter
import otf2
import igraph as ig
import otter
from otter import args
from otter.log import DEBUG
from otter.utils import VertexLabeller, AttributeHandlerTable, VertexAttributeCombiner
from otter.definitions import RegionType, Endpoint

log = otter.log.get_logger("main")

log.info(f"reading OTF2 anchorfile: {args.anchorfile}")
with otf2.reader.open(args.anchorfile) as r:
    events = otter.EventFactory(r)
    tasks = otter.TaskRegistry()
    log.info(f"generating chunks")
    chunks = otter.ChunkFactory(events, tasks).chunks
    graphs = list(chunk.graph for chunk in chunks)

# Dump chunks and graphs to log file
if args.loglevel == "DEBUG":
    chunk_log = otter.log.get_logger("chunks_debug")
    graph_log = otter.log.get_logger("graphs_debug")

    graph_log.debug(">>> BEGIN GRAPHS <<<")
    chunk_log.debug(f">>> BEGIN CHUNKS <<<")

    for chunk in chunks:

        # write chunk
        for line in chunk.to_text():
            chunk_log.debug(f"{line}")

        # write graph
        graph_log.debug(f"Chunk type: {chunk.type}")
        g = chunk.graph
        lines = [" ".join(f"{g}".split("\n"))]
        for line in lines:
            graph_log.debug(f"{line}")
        for v in g.vs:
            graph_log.debug(f"{v}")
        graph_log.debug("")

    chunk_log.debug(f">>> END CHUNKS <<<")
    graph_log.debug(">>> END GRAPHS <<<")

# Collect all chunks
log.info("combining chunks")
g = ig.disjoint_union([c.graph for c in chunks])
vcount = g.vcount()
log.info(f"graph disjoint union has {vcount} vertices")

# Define some vertex attributes
for name in ['_task_cluster_id', '_is_task_enter_node', '_is_task_leave_node', '_region_type', '_master_enter_event']:
    if name not in g.vs.attribute_names():
        g.vs[name] = None
# g.vs['_synchronised_by_taskwait'] = False

# Create vertex labellers
log.info("creating vertex labellers")
parallel_vertex_labeller = VertexLabeller(otter.utils.key_is_not_none('_parallel_sequence_id'), group_key='_parallel_sequence_id')
single_vertex_labeller = VertexLabeller(otter.utils.is_region_type(RegionType.single_executor), group_key='event')
master_vertex_labeller = VertexLabeller(otter.utils.is_region_type(RegionType.master), group_key='event')
task_vertex_labeller = VertexLabeller(otter.utils.key_is_not_none('_task_cluster_id'), group_key='_task_cluster_id')
empty_task_vertex_labeller = VertexLabeller(otter.utils.is_empty_task_region, group_key=lambda v: v['_task_cluster_id'][0])

# Make a table for mapping vertex attributes to handlers - used by ig.Graph.contract_vertices
handlers = AttributeHandlerTable(g.vs.attribute_names(), level=DEBUG)

# Supply the logic to use when combining each of these vertex attributes
attribute_handlers = [
    ("_master_enter_event", otter.utils.handlers.return_unique_master_event, (type(None), otter.events._Event)),
    ("_task_cluster_id",    otter.utils.handlers.pass_the_unique_value,      (type(None), tuple)),
    ("_is_task_enter_node", otter.utils.handlers.pass_bool_value,            (type(None), bool)),
    ("_is_task_leave_node", otter.utils.handlers.pass_bool_value,            (type(None), bool))
]
for attribute, handler, accept in attribute_handlers:
    handlers[attribute] = VertexAttributeCombiner(handler, accept=accept, msg=f"combining attribute: {attribute}")

log.info(f"combining vertices...")

"""
Contract vertices according to _parallel_sequence_id to combine the chunks generated by the threads of a parallel block.
When combining the 'event' vertex attribute, keep single-executor events over single-other events. All other events
should be combined in a list. 
"""
log.info(f"combining vertices by parallel sequence ID")
handlers['event'] = VertexAttributeCombiner(otter.utils.handlers.return_unique_single_executor_event)
labeller = VertexLabeller(otter.utils.key_is_not_none('_parallel_sequence_id'), group_key='_parallel_sequence_id')
g.contract_vertices(labeller.label(g.vs), combine_attrs=handlers)
vcount_prev, vcount = vcount, g.vcount()
log.info(f"vertex count updated: {vcount_prev} -> {vcount}")

"""
Contract those vertices which refer to the same single-executor event. This connects single-executor chunks to the
chunks containing them, as both chunks contain references to the single-exec-begin/end events.
"""
log.info(f"combining vertices by single-begin/end event")
labeller = VertexLabeller(otter.utils.is_region_type(RegionType.single_executor), group_key='event')
g.contract_vertices(labeller.label(g.vs), combine_attrs=handlers)
vcount_prev, vcount = vcount, g.vcount()
log.info(f"vertex count updated: {vcount_prev} -> {vcount}")

"""
master-leave vertices (which refer to their master-leave event) refer to their corresponding master-enter event.
"""
log.info(f"combining vertices by master-begin/end event")
handlers['event'] = VertexAttributeCombiner(otter.utils.handlers.return_unique_master_event)
labeller = VertexLabeller(otter.utils.is_region_type(RegionType.master), group_key='event')
g.contract_vertices(labeller.label(g.vs), combine_attrs=handlers)
vcount_prev, vcount = vcount, g.vcount()
log.info(f"vertex count updated: {vcount_prev} -> {vcount}")


"""
Intermediate clean-up: for each master region, remove edges that connect
the same nodes as the master region
"""
master_enter_vertices = filter(lambda vertex: isinstance(vertex['event'], otter.events.MasterBegin), g.vs)
master_leave_vertices = filter(lambda vertex: isinstance(vertex['event'], otter.events.MasterEnd), g.vs)
master_enter_vertex_map = {enter_vertex['event']: enter_vertex for enter_vertex in master_enter_vertices}
master_vertex_pairs = ((master_enter_vertex_map[leave_vertex['_master_enter_event']], leave_vertex) for leave_vertex in master_leave_vertices)
neighbour_pairs = {(enter.predecessors()[0], leave.successors()[0]) for enter, leave in master_vertex_pairs}
redundant_edges = list(filter(lambda edge: (edge.source_vertex, edge.target_vertex) in neighbour_pairs, g.es))
log.info(f"deleting redundant edges due to master regions: {len(redundant_edges)}")
g.delete_edges(redundant_edges)

"""
Contract by _task_cluster_id, rejecting task-create vertices to replace task-create vertices with the corresponding
task's chunk.
"""
log.info("combining vertices by task ID & endpoint")
handlers['event'] = VertexAttributeCombiner(otter.utils.handlers.reject_task_create)
labeller = VertexLabeller(otter.utils.key_is_not_none('_task_cluster_id'), group_key='_task_cluster_id')
g.contract_vertices(labeller.label(g.vs), combine_attrs=handlers)
vcount_prev, vcount = vcount, g.vcount()
log.info(f"vertex count updated: {vcount_prev} -> {vcount}")

"""
Contract vertices with the same task ID where the task chunk contains no internal vertices to get 1 vertex per empty
task region.
"""
log.info("combining vertices by task ID where there are no nested nodes")
handlers['_task_cluster_id'] = VertexAttributeCombiner(otter.utils.handlers.pass_the_set_of_values, accept=tuple, msg="combining attribute: _task_cluster_id")
labeller = VertexLabeller(otter.utils.is_empty_task_region, group_key=lambda v: v['_task_cluster_id'][0])
g.contract_vertices(labeller.label(g.vs), combine_attrs=handlers)
vcount_prev, vcount = vcount, g.vcount()
log.info(f"vertex count updated: {vcount_prev} -> {vcount}")

# Label (contracted) task nodes for easier identification
# NOTE: don't think this is currently needed - leave here for reference
# for v in g.vs:
#     if v['is_task_enter_node'] and v['is_task_leave_node']:
#         v['is_contracted_task_node'] = True
#         if type(v['event']) is list and len(v['event']) == 0:
#             pdb.set_trace()
#         v['task_id'] = event_attr(v['event'], 'unique_id')
#     elif v['is_task_enter_node'] or v['is_task_leave_node']:
#         v['task_id'] = v['task_cluster_id'][0]

# Define _sync_cluster_id to identify pairs of connected nodes to contract
# the dummy value is used as a label to contract vertices
dummy_counter = count()
g.vs['_sync_cluster_id'] = None
for edge in g.es:
    if otter.utils.edge_connects_same_type(edge, [RegionType.barrier_implicit, RegionType.barrier_explicit, RegionType.taskwait, RegionType.loop]):
        edge.source_vertex['_sync_cluster_id'] = edge.target_vertex['_sync_cluster_id'] = next(dummy_counter)

"""
Contract pairs of directly connected vertices which represent empty barriers, taskwait & loop regions.  
"""
log.info("combining redundant sync and loop enter/leave node pairs")
handlers['event'] = VertexAttributeCombiner(otter.utils.handlers.pass_args)
labeller= VertexLabeller(otter.utils.key_is_not_none('_sync_cluster_id'), group_key='_sync_cluster_id')
g.contract_vertices(labeller.label(g.vs), combine_attrs=handlers)
vcount_prev, vcount = vcount, g.vcount()
log.info(f"vertex count updated: {vcount_prev} -> {vcount}")

# Unpack the region_type attribute


"""
Apply taskwait synchronisation
"""
log.info("applying taskwait synchronisation")

"""
1. Give each explicit task a reference to the last vertex which represents its chunk. This is the vertex which will be
connected to the corresponding taskwait vertex, if applicable. The correct vertex will contain exactly 1 TaskSwitch(complete)
event for the corresponding task.
"""
log.debug(f"filtering for task-complete vertices")
task_complete_vertices = dict()
for vertex in filter(otter.utils.is_terminal_task_vertex, g.vs):
    event = vertex['event']
    if isinstance(event, list):
        event = otter.utils.handlers.return_unique_taskswitch_complete_event(event)
    assert otter.events.is_event(event)
    log.debug(f" - task {event.prior_task_id}: {event}")
    task_complete_vertices[event.prior_task_id] = vertex

"""
2. For each task that encountered a taskwait barrier, make a list of the taskwait-begin/end event pairs & the vertex
which refers to these events
"""
log.debug(f"gathering taskwait vertices by encountering task ID")
taskwait_vertices = defaultdict(list)
for vertex in filter(otter.utils.is_region_type(RegionType.taskwait), g.vs):
    log.debug(f"got taskwait vertex {vertex} with events:")
    for event in vertex['event']: # now guaranteed to have exactly 2 event instances per vertex (tw-begin+end)
        log.debug(f" - {event}")
        taskwait_vertices[event.encountering_task_id].append((event, vertex))

"""
3. For each task that encountered a taskwait barrier, connect child tasks to the correct taskwait barrier (if any).
"""
for taskID, event_vertex_pairs in taskwait_vertices.items():

    log.debug(f"applying taskwait synchronisation for children of task {taskID}: {list(tasks[taskID].children)}")

    """
    Keep the event-vertex pairs for which the event is the taskwait-enter event
    """
    event_vertex_pairs = sorted(filter(lambda p: p[0].endpoint == Endpoint.enter, event_vertex_pairs), key=lambda p: p[0].time)
    log.debug(f"task {taskID} encountered {(len(event_vertex_pairs))} taskwait barriers:")
    for event, vertex in event_vertex_pairs:
        log.debug(f" - task {taskID} encountered {event} {vertex}")

    """Iterate over the taskwait-enter events for this taskID, in chronological order"""
    event_vertex_pairs_iter = iter(event_vertex_pairs)

    """Iterate over the children of this task in the order they were created"""
    children_iter = iter(sorted(tasks[taskID].children, key=lambda id: tasks[id].crt_ts))

    """Get the first pair of taskwait-enter events"""
    (previous_event, previous_vertex), (next_event, next_vertex) = (None, None), next(event_vertex_pairs_iter, (None, None))

    while True:

        """Get the next child task if any are left to be synchronised"""
        try:
            child = next(children_iter)
        except StopIteration:
            """ran out of child tasks"""
            break

        """Assert: expect that we haven't already missed the taskwait barrier for this child task"""
        if previous_event and tasks[child].crt_ts < previous_event.time:
            print(tasks[child])
            print(previous_event)
            raise ValueError("child created before previous")

        """This child is synchronised by either "next_event" or a subsequent taskwait barrier"""
        while next_event and next_event.time <= tasks[child].crt_ts:
            (previous_event, previous_vertex), (next_event, next_vertex) = (next_event, next_vertex), next(event_vertex_pairs_iter, (None, None))

        task_complete_vertex = task_complete_vertices[child]
        del task_complete_vertices[child]
        assert next_event.time > tasks[child].crt_ts and (previous_event is None or previous_event.time < tasks[child].crt_ts)
        edge = g.add_edge(task_complete_vertex, next_vertex)
        edge['taskwait'] = True
        log.debug(f"synchronised task {child}:")
        log.debug(f"  from: {task_complete_vertex}")
        log.debug(f"    to: {next_vertex}")

        if previous_event is None and next_event is None:
            """ran out of taskwait barriers - no further taskwait synchronisation to apply to children of this task"""
            break



"""
Apply taskgroup synchronisation:
1. Get all taskwait nodes
2. Get all events that encountered taskwait nodes
3. Find the children of each event that encountered taskwait nodes
4. 

==> Get all tasks which encountered a taskwait at some point
==> For each such task:
==>     Get its children in order of their task-create times
==>     Order the taskwait barriers in the order they were encountered
==>     For each taskwait barrier encountered:
==>         Assign each child which was created before the taskwait was encountered to that taskwait barrier
"""

g.simplify(combine_edges='first')

if args.output:
    log.info(f"writing graph to '{args.output}'")
    import warnings
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")
        try:
            g.write(args.output)
        except OSError as oserr:
            print(f"igraph error: {oserr}")
            print(f"failed to write to file '{args.output}'")

with open("graph.log", "w") as f:
    f.write("### VERTICES:\n")
    for v in g.vs:
        f.write(f"{v}\n")

    f.write("\n")
    f.write("### EDGES:\n")
    for e in g.es:
        f.write(f"{e.tuple}\n")

log.info("Done!")
